{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e935bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "# %%\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from utilities import WaveformDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a446225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bf3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61017771",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879dbac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d034156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Read the pre-processed datasets\n",
    "model_datasets = '../training_datasets/training_datasets_STEAD_waveform.hdf5'\n",
    "with h5py.File(model_datasets, 'r') as f:\n",
    "    X_train = f['X_train'][:]\n",
    "    Y_train = f['Y_train'][:]\n",
    "\n",
    "# 3. split to training (60%), validation (20%) and test (20%)\n",
    "train_size = 0.6\n",
    "rand_seed1 = 13\n",
    "rand_seed2 = 20\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, train_size=0.1, random_state=rand_seed1)\n",
    "X_validate, X_test, Y_validate, Y_test = train_test_split(X_test, Y_test, test_size=0.9, random_state=rand_seed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91ed800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to the dataset class for Pytorch (here simply load all the data,\n",
    "# but for the sake of memory, can also use WaveformDataset_h5)\n",
    "training_data = WaveformDataset(X_train, Y_train)\n",
    "validate_data = WaveformDataset(X_validate, Y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0efb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mini-batch (testing)\n",
    "from torch.utils.data import DataLoader\n",
    "from autoencoder_1D_models_torch import Autoencoder_Conv1D_deep\n",
    "\n",
    "def training_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss= 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Avg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bef157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " ===========================\n",
      "loss: 2.000715 [    0/10000]\n",
      "loss: 1.287735 [ 2560/10000]\n",
      "loss: 0.990403 [ 5120/10000]\n",
      "loss: 0.848007 [ 7680/10000]\n",
      "Test Avg loss: 0.761452\n",
      "\n",
      "Epoch 2\n",
      " ===========================\n",
      "loss: 0.751938 [    0/10000]\n",
      "loss: 0.678050 [ 2560/10000]\n",
      "loss: 0.573800 [ 5120/10000]\n",
      "loss: 0.500830 [ 7680/10000]\n",
      "Test Avg loss: 0.449007\n",
      "\n",
      "Epoch 3\n",
      " ===========================\n",
      "loss: 0.452714 [    0/10000]\n",
      "loss: 0.393113 [ 2560/10000]\n",
      "loss: 0.348840 [ 5120/10000]\n",
      "loss: 0.336185 [ 7680/10000]\n",
      "Test Avg loss: 0.313086\n",
      "\n",
      "Epoch 4\n",
      " ===========================\n",
      "loss: 0.321363 [    0/10000]\n",
      "loss: 0.279932 [ 2560/10000]\n",
      "loss: 0.256149 [ 5120/10000]\n",
      "loss: 0.210433 [ 7680/10000]\n",
      "Test Avg loss: 0.186103\n",
      "\n",
      "Epoch 5\n",
      " ===========================\n",
      "loss: 0.195432 [    0/10000]\n",
      "loss: 0.151620 [ 2560/10000]\n",
      "loss: 0.150982 [ 5120/10000]\n",
      "loss: 0.131150 [ 7680/10000]\n",
      "Test Avg loss: 0.113603\n",
      "\n",
      "Epoch 6\n",
      " ===========================\n",
      "loss: 0.112633 [    0/10000]\n",
      "loss: 0.105765 [ 2560/10000]\n",
      "loss: 0.078263 [ 5120/10000]\n",
      "loss: 0.074354 [ 7680/10000]\n",
      "Test Avg loss: 0.067429\n",
      "\n",
      "Epoch 7\n",
      " ===========================\n",
      "loss: 0.071728 [    0/10000]\n",
      "loss: 0.059278 [ 2560/10000]\n",
      "loss: 0.050724 [ 5120/10000]\n",
      "loss: 0.050109 [ 7680/10000]\n",
      "Test Avg loss: 0.041784\n",
      "\n",
      "Epoch 8\n",
      " ===========================\n",
      "loss: 0.035755 [    0/10000]\n",
      "loss: 0.038254 [ 2560/10000]\n",
      "loss: 0.032098 [ 5120/10000]\n",
      "loss: 0.030652 [ 7680/10000]\n",
      "Test Avg loss: 0.032853\n",
      "\n",
      "Epoch 9\n",
      " ===========================\n",
      "loss: 0.029653 [    0/10000]\n",
      "loss: 0.028352 [ 2560/10000]\n",
      "loss: 0.025497 [ 5120/10000]\n",
      "loss: 0.025684 [ 7680/10000]\n",
      "Test Avg loss: 0.025164\n",
      "\n",
      "Epoch 10\n",
      " ===========================\n",
      "loss: 0.034868 [    0/10000]\n",
      "loss: 0.016287 [ 2560/10000]\n",
      "loss: 0.021367 [ 5120/10000]\n",
      "loss: 0.015467 [ 7680/10000]\n",
      "Test Avg loss: 0.020587\n",
      "\n",
      "Epoch 11\n",
      " ===========================\n",
      "loss: 0.018787 [    0/10000]\n",
      "loss: 0.024715 [ 2560/10000]\n",
      "loss: 0.016263 [ 5120/10000]\n",
      "loss: 0.017677 [ 7680/10000]\n",
      "Test Avg loss: 0.021217\n",
      "\n",
      "Epoch 12\n",
      " ===========================\n",
      "loss: 0.019258 [    0/10000]\n",
      "loss: 0.016246 [ 2560/10000]\n",
      "loss: 0.019727 [ 5120/10000]\n",
      "loss: 0.018179 [ 7680/10000]\n",
      "Test Avg loss: 0.025602\n",
      "\n",
      "Epoch 13\n",
      " ===========================\n",
      "loss: 0.023360 [    0/10000]\n",
      "loss: 0.034042 [ 2560/10000]\n",
      "loss: 0.021460 [ 5120/10000]\n",
      "loss: 0.019739 [ 7680/10000]\n",
      "Test Avg loss: 0.021842\n",
      "\n",
      "Epoch 14\n",
      " ===========================\n",
      "loss: 0.015938 [    0/10000]\n",
      "loss: 0.021038 [ 2560/10000]\n",
      "loss: 0.015834 [ 5120/10000]\n",
      "loss: 0.019911 [ 7680/10000]\n",
      "Test Avg loss: 0.020418\n",
      "\n",
      "Epoch 15\n",
      " ===========================\n",
      "loss: 0.021629 [    0/10000]\n",
      "loss: 0.015901 [ 2560/10000]\n",
      "loss: 0.019884 [ 5120/10000]\n",
      "loss: 0.017679 [ 7680/10000]\n",
      "Test Avg loss: 0.015565\n",
      "\n",
      "Epoch 16\n",
      " ===========================\n",
      "loss: 0.017930 [    0/10000]\n",
      "loss: 0.017658 [ 2560/10000]\n",
      "loss: 0.014647 [ 5120/10000]\n",
      "loss: 0.012561 [ 7680/10000]\n",
      "Test Avg loss: 0.019428\n",
      "\n",
      "Epoch 17\n",
      " ===========================\n",
      "loss: 0.018422 [    0/10000]\n",
      "loss: 0.013430 [ 2560/10000]\n",
      "loss: 0.015249 [ 5120/10000]\n",
      "loss: 0.016780 [ 7680/10000]\n",
      "Test Avg loss: 0.014564\n",
      "\n",
      "Epoch 18\n",
      " ===========================\n",
      "loss: 0.013192 [    0/10000]\n",
      "loss: 0.011202 [ 2560/10000]\n",
      "loss: 0.009683 [ 5120/10000]\n",
      "loss: 0.012079 [ 7680/10000]\n",
      "Test Avg loss: 0.014298\n",
      "\n",
      "Epoch 19\n",
      " ===========================\n",
      "loss: 0.010922 [    0/10000]\n",
      "loss: 0.012149 [ 2560/10000]\n",
      "loss: 0.014060 [ 5120/10000]\n",
      "loss: 0.011849 [ 7680/10000]\n",
      "Test Avg loss: 0.013468\n",
      "\n",
      "Epoch 20\n",
      " ===========================\n",
      "loss: 0.014129 [    0/10000]\n",
      "loss: 0.009784 [ 2560/10000]\n",
      "loss: 0.011727 [ 5120/10000]\n",
      "loss: 0.012256 [ 7680/10000]\n",
      "Test Avg loss: 0.021205\n",
      "\n",
      "Epoch 21\n",
      " ===========================\n",
      "loss: 0.018556 [    0/10000]\n",
      "loss: 0.017175 [ 2560/10000]\n",
      "loss: 0.016068 [ 5120/10000]\n",
      "loss: 0.010291 [ 7680/10000]\n",
      "Test Avg loss: 0.016808\n",
      "\n",
      "Epoch 22\n",
      " ===========================\n",
      "loss: 0.016425 [    0/10000]\n",
      "loss: 0.009135 [ 2560/10000]\n",
      "loss: 0.010157 [ 5120/10000]\n",
      "loss: 0.010530 [ 7680/10000]\n",
      "Test Avg loss: 0.012814\n",
      "\n",
      "Epoch 23\n",
      " ===========================\n",
      "loss: 0.013544 [    0/10000]\n",
      "loss: 0.008935 [ 2560/10000]\n",
      "loss: 0.010782 [ 5120/10000]\n",
      "loss: 0.012569 [ 7680/10000]\n",
      "Test Avg loss: 0.011792\n",
      "\n",
      "Epoch 24\n",
      " ===========================\n",
      "loss: 0.008794 [    0/10000]\n",
      "loss: 0.011031 [ 2560/10000]\n",
      "loss: 0.008835 [ 5120/10000]\n",
      "loss: 0.012048 [ 7680/10000]\n",
      "Test Avg loss: 0.013505\n",
      "\n",
      "Epoch 25\n",
      " ===========================\n",
      "loss: 0.012145 [    0/10000]\n",
      "loss: 0.008362 [ 2560/10000]\n",
      "loss: 0.009361 [ 5120/10000]\n",
      "loss: 0.008475 [ 7680/10000]\n",
      "Test Avg loss: 0.012386\n",
      "\n",
      "Epoch 26\n",
      " ===========================\n",
      "loss: 0.010171 [    0/10000]\n",
      "loss: 0.009558 [ 2560/10000]\n",
      "loss: 0.010922 [ 5120/10000]\n",
      "loss: 0.009293 [ 7680/10000]\n",
      "Test Avg loss: 0.011233\n",
      "\n",
      "Epoch 27\n",
      " ===========================\n",
      "loss: 0.012192 [    0/10000]\n",
      "loss: 0.008642 [ 2560/10000]\n",
      "loss: 0.008786 [ 5120/10000]\n",
      "loss: 0.011604 [ 7680/10000]\n",
      "Test Avg loss: 0.011200\n",
      "\n",
      "Epoch 28\n",
      " ===========================\n",
      "loss: 0.009192 [    0/10000]\n",
      "loss: 0.009598 [ 2560/10000]\n",
      "loss: 0.012294 [ 5120/10000]\n",
      "loss: 0.009707 [ 7680/10000]\n",
      "Test Avg loss: 0.008986\n",
      "\n",
      "Epoch 29\n",
      " ===========================\n",
      "loss: 0.008717 [    0/10000]\n",
      "loss: 0.011332 [ 2560/10000]\n",
      "loss: 0.010467 [ 5120/10000]\n",
      "loss: 0.012414 [ 7680/10000]\n",
      "Test Avg loss: 0.009619\n",
      "\n",
      "Epoch 30\n",
      " ===========================\n",
      "loss: 0.014305 [    0/10000]\n",
      "loss: 0.011061 [ 2560/10000]\n",
      "loss: 0.008021 [ 5120/10000]\n",
      "loss: 0.006663 [ 7680/10000]\n",
      "Test Avg loss: 0.009291\n",
      "\n",
      "Epoch 31\n",
      " ===========================\n",
      "loss: 0.007323 [    0/10000]\n",
      "loss: 0.010341 [ 2560/10000]\n",
      "loss: 0.007368 [ 5120/10000]\n",
      "loss: 0.007284 [ 7680/10000]\n",
      "Test Avg loss: 0.009122\n",
      "\n",
      "Epoch 32\n",
      " ===========================\n",
      "loss: 0.008555 [    0/10000]\n",
      "loss: 0.006111 [ 2560/10000]\n",
      "loss: 0.009303 [ 5120/10000]\n",
      "loss: 0.012193 [ 7680/10000]\n",
      "Test Avg loss: 0.009237\n",
      "\n",
      "Epoch 33\n",
      " ===========================\n",
      "loss: 0.010021 [    0/10000]\n",
      "loss: 0.006633 [ 2560/10000]\n",
      "loss: 0.009004 [ 5120/10000]\n",
      "loss: 0.008732 [ 7680/10000]\n",
      "Test Avg loss: 0.008744\n",
      "\n",
      "Epoch 34\n",
      " ===========================\n",
      "loss: 0.007453 [    0/10000]\n",
      "loss: 0.008622 [ 2560/10000]\n",
      "loss: 0.015857 [ 5120/10000]\n",
      "loss: 0.006956 [ 7680/10000]\n",
      "Test Avg loss: 0.008907\n",
      "\n",
      "Epoch 35\n",
      " ===========================\n",
      "loss: 0.006860 [    0/10000]\n",
      "loss: 0.012253 [ 2560/10000]\n",
      "loss: 0.009843 [ 5120/10000]\n",
      "loss: 0.011595 [ 7680/10000]\n",
      "Test Avg loss: 0.008755\n",
      "\n",
      "Epoch 36\n",
      " ===========================\n",
      "loss: 0.006687 [    0/10000]\n",
      "loss: 0.006038 [ 2560/10000]\n",
      "loss: 0.010473 [ 5120/10000]\n",
      "loss: 0.007141 [ 7680/10000]\n",
      "Test Avg loss: 0.012772\n",
      "\n",
      "Epoch 37\n",
      " ===========================\n",
      "loss: 0.017340 [    0/10000]\n",
      "loss: 0.015171 [ 2560/10000]\n",
      "loss: 0.009939 [ 5120/10000]\n",
      "loss: 0.009558 [ 7680/10000]\n",
      "Test Avg loss: 0.011722\n",
      "\n",
      "Epoch 38\n",
      " ===========================\n",
      "loss: 0.010230 [    0/10000]\n",
      "loss: 0.011583 [ 2560/10000]\n",
      "loss: 0.008576 [ 5120/10000]\n",
      "loss: 0.008986 [ 7680/10000]\n",
      "Test Avg loss: 0.010120\n",
      "\n",
      "Epoch 39\n",
      " ===========================\n",
      "loss: 0.008682 [    0/10000]\n",
      "loss: 0.006111 [ 2560/10000]\n",
      "loss: 0.008423 [ 5120/10000]\n",
      "loss: 0.006941 [ 7680/10000]\n",
      "Test Avg loss: 0.008578\n",
      "\n",
      "Epoch 40\n",
      " ===========================\n",
      "loss: 0.009461 [    0/10000]\n",
      "loss: 0.010170 [ 2560/10000]\n",
      "loss: 0.006521 [ 5120/10000]\n",
      "loss: 0.007479 [ 7680/10000]\n",
      "Test Avg loss: 0.007549\n",
      "\n",
      "Epoch 41\n",
      " ===========================\n",
      "loss: 0.005551 [    0/10000]\n",
      "loss: 0.008708 [ 2560/10000]\n",
      "loss: 0.005798 [ 5120/10000]\n",
      "loss: 0.007073 [ 7680/10000]\n",
      "Test Avg loss: 0.008634\n",
      "\n",
      "Epoch 42\n",
      " ===========================\n",
      "loss: 0.010037 [    0/10000]\n",
      "loss: 0.006627 [ 2560/10000]\n",
      "loss: 0.005953 [ 5120/10000]\n",
      "loss: 0.006482 [ 7680/10000]\n",
      "Test Avg loss: 0.009361\n",
      "\n",
      "Epoch 43\n",
      " ===========================\n",
      "loss: 0.012925 [    0/10000]\n",
      "loss: 0.010244 [ 2560/10000]\n",
      "loss: 0.008227 [ 5120/10000]\n",
      "loss: 0.005256 [ 7680/10000]\n",
      "Test Avg loss: 0.008241\n",
      "\n",
      "Epoch 44\n",
      " ===========================\n",
      "loss: 0.008182 [    0/10000]\n",
      "loss: 0.005501 [ 2560/10000]\n",
      "loss: 0.006685 [ 5120/10000]\n",
      "loss: 0.007834 [ 7680/10000]\n",
      "Test Avg loss: 0.007568\n",
      "\n",
      "Epoch 45\n",
      " ===========================\n",
      "loss: 0.005535 [    0/10000]\n",
      "loss: 0.006898 [ 2560/10000]\n",
      "loss: 0.006233 [ 5120/10000]\n",
      "loss: 0.008671 [ 7680/10000]\n",
      "Test Avg loss: 0.009910\n",
      "\n",
      "Epoch 46\n",
      " ===========================\n",
      "loss: 0.008414 [    0/10000]\n",
      "loss: 0.007851 [ 2560/10000]\n",
      "loss: 0.004190 [ 5120/10000]\n",
      "loss: 0.008817 [ 7680/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Avg loss: 0.007190\n",
      "\n",
      "Epoch 47\n",
      " ===========================\n",
      "loss: 0.006711 [    0/10000]\n",
      "loss: 0.005609 [ 2560/10000]\n",
      "loss: 0.009005 [ 5120/10000]\n",
      "loss: 0.006028 [ 7680/10000]\n",
      "Test Avg loss: 0.006612\n",
      "\n",
      "Epoch 48\n",
      " ===========================\n",
      "loss: 0.008152 [    0/10000]\n",
      "loss: 0.006465 [ 2560/10000]\n",
      "loss: 0.006727 [ 5120/10000]\n",
      "loss: 0.005948 [ 7680/10000]\n",
      "Test Avg loss: 0.007771\n",
      "\n",
      "Epoch 49\n",
      " ===========================\n",
      "loss: 0.006667 [    0/10000]\n",
      "loss: 0.007474 [ 2560/10000]\n",
      "loss: 0.007459 [ 5120/10000]\n",
      "loss: 0.006075 [ 7680/10000]\n",
      "Test Avg loss: 0.010729\n",
      "\n",
      "Epoch 50\n",
      " ===========================\n",
      "loss: 0.008844 [    0/10000]\n",
      "loss: 0.009420 [ 2560/10000]\n",
      "loss: 0.006954 [ 5120/10000]\n",
      "loss: 0.006885 [ 7680/10000]\n",
      "Test Avg loss: 0.007613\n",
      "\n",
      "Epoch 51\n",
      " ===========================\n",
      "loss: 0.007585 [    0/10000]\n",
      "loss: 0.008125 [ 2560/10000]\n",
      "loss: 0.006046 [ 5120/10000]\n",
      "loss: 0.006239 [ 7680/10000]\n",
      "Test Avg loss: 0.006356\n",
      "\n",
      "Epoch 52\n",
      " ===========================\n",
      "loss: 0.005661 [    0/10000]\n",
      "loss: 0.009254 [ 2560/10000]\n",
      "loss: 0.008121 [ 5120/10000]\n",
      "loss: 0.007956 [ 7680/10000]\n",
      "Test Avg loss: 0.007979\n",
      "\n",
      "Epoch 53\n",
      " ===========================\n",
      "loss: 0.009786 [    0/10000]\n",
      "loss: 0.008434 [ 2560/10000]\n",
      "loss: 0.006367 [ 5120/10000]\n",
      "loss: 0.007247 [ 7680/10000]\n",
      "Test Avg loss: 0.008375\n",
      "\n",
      "Epoch 54\n",
      " ===========================\n",
      "loss: 0.007484 [    0/10000]\n",
      "loss: 0.007551 [ 2560/10000]\n",
      "loss: 0.007051 [ 5120/10000]\n",
      "loss: 0.007923 [ 7680/10000]\n",
      "Test Avg loss: 0.007836\n",
      "\n",
      "Epoch 55\n",
      " ===========================\n",
      "loss: 0.008629 [    0/10000]\n",
      "loss: 0.006422 [ 2560/10000]\n",
      "loss: 0.004259 [ 5120/10000]\n",
      "loss: 0.004953 [ 7680/10000]\n",
      "Test Avg loss: 0.007764\n",
      "\n",
      "Epoch 56\n",
      " ===========================\n",
      "loss: 0.005483 [    0/10000]\n",
      "loss: 0.004584 [ 2560/10000]\n",
      "loss: 0.005089 [ 5120/10000]\n",
      "loss: 0.007371 [ 7680/10000]\n",
      "Test Avg loss: 0.007503\n",
      "\n",
      "Epoch 57\n",
      " ===========================\n",
      "loss: 0.007427 [    0/10000]\n",
      "loss: 0.004588 [ 2560/10000]\n",
      "loss: 0.007588 [ 5120/10000]\n",
      "loss: 0.006272 [ 7680/10000]\n",
      "Test Avg loss: 0.006894\n",
      "\n",
      "Epoch 58\n",
      " ===========================\n",
      "loss: 0.005403 [    0/10000]\n",
      "loss: 0.010693 [ 2560/10000]\n",
      "loss: 0.007076 [ 5120/10000]\n",
      "loss: 0.005523 [ 7680/10000]\n",
      "Test Avg loss: 0.006272\n",
      "\n",
      "Epoch 59\n",
      " ===========================\n",
      "loss: 0.004535 [    0/10000]\n",
      "loss: 0.004201 [ 2560/10000]\n",
      "loss: 0.006807 [ 5120/10000]\n",
      "loss: 0.003737 [ 7680/10000]\n",
      "Test Avg loss: 0.006414\n",
      "\n",
      "Epoch 60\n",
      " ===========================\n",
      "loss: 0.005028 [    0/10000]\n",
      "loss: 0.005078 [ 2560/10000]\n",
      "loss: 0.005783 [ 5120/10000]\n",
      "loss: 0.004479 [ 7680/10000]\n",
      "Test Avg loss: 0.007303\n",
      "\n",
      "Epoch 61\n",
      " ===========================\n",
      "loss: 0.006196 [    0/10000]\n",
      "loss: 0.003578 [ 2560/10000]\n",
      "loss: 0.003082 [ 5120/10000]\n",
      "loss: 0.005479 [ 7680/10000]\n",
      "Test Avg loss: 0.006170\n",
      "\n",
      "Epoch 62\n",
      " ===========================\n",
      "loss: 0.006601 [    0/10000]\n",
      "loss: 0.005237 [ 2560/10000]\n",
      "loss: 0.006077 [ 5120/10000]\n",
      "loss: 0.005111 [ 7680/10000]\n",
      "Test Avg loss: 0.007866\n",
      "\n",
      "Epoch 63\n",
      " ===========================\n",
      "loss: 0.007284 [    0/10000]\n",
      "loss: 0.004552 [ 2560/10000]\n",
      "loss: 0.005725 [ 5120/10000]\n",
      "loss: 0.005721 [ 7680/10000]\n",
      "Test Avg loss: 0.006840\n",
      "\n",
      "Epoch 64\n",
      " ===========================\n",
      "loss: 0.005603 [    0/10000]\n",
      "loss: 0.005389 [ 2560/10000]\n",
      "loss: 0.011198 [ 5120/10000]\n",
      "loss: 0.005882 [ 7680/10000]\n",
      "Test Avg loss: 0.007024\n",
      "\n",
      "Epoch 65\n",
      " ===========================\n",
      "loss: 0.007246 [    0/10000]\n",
      "loss: 0.005330 [ 2560/10000]\n",
      "loss: 0.007779 [ 5120/10000]\n",
      "loss: 0.004503 [ 7680/10000]\n",
      "Test Avg loss: 0.011841\n",
      "\n",
      "Epoch 66\n",
      " ===========================\n",
      "loss: 0.012175 [    0/10000]\n",
      "loss: 0.011206 [ 2560/10000]\n",
      "loss: 0.007021 [ 5120/10000]\n",
      "loss: 0.007363 [ 7680/10000]\n",
      "Test Avg loss: 0.006379\n",
      "\n",
      "Epoch 67\n",
      " ===========================\n",
      "loss: 0.005477 [    0/10000]\n",
      "loss: 0.005609 [ 2560/10000]\n",
      "loss: 0.004240 [ 5120/10000]\n",
      "loss: 0.007989 [ 7680/10000]\n",
      "Test Avg loss: 0.006121\n",
      "\n",
      "Epoch 68\n",
      " ===========================\n",
      "loss: 0.005989 [    0/10000]\n",
      "loss: 0.005236 [ 2560/10000]\n",
      "loss: 0.006179 [ 5120/10000]\n",
      "loss: 0.006628 [ 7680/10000]\n",
      "Test Avg loss: 0.007198\n",
      "\n",
      "Epoch 69\n",
      " ===========================\n",
      "loss: 0.009395 [    0/10000]\n",
      "loss: 0.007663 [ 2560/10000]\n",
      "loss: 0.006844 [ 5120/10000]\n",
      "loss: 0.005390 [ 7680/10000]\n",
      "Test Avg loss: 0.006844\n",
      "\n",
      "Epoch 70\n",
      " ===========================\n",
      "loss: 0.009596 [    0/10000]\n",
      "loss: 0.006077 [ 2560/10000]\n",
      "loss: 0.005252 [ 5120/10000]\n",
      "loss: 0.008189 [ 7680/10000]\n",
      "Test Avg loss: 0.006400\n",
      "\n",
      "Epoch 71\n",
      " ===========================\n",
      "loss: 0.005484 [    0/10000]\n",
      "loss: 0.005471 [ 2560/10000]\n",
      "loss: 0.007044 [ 5120/10000]\n",
      "loss: 0.005229 [ 7680/10000]\n",
      "Test Avg loss: 0.006491\n",
      "\n",
      "Epoch 72\n",
      " ===========================\n",
      "loss: 0.009096 [    0/10000]\n",
      "loss: 0.004799 [ 2560/10000]\n",
      "loss: 0.005775 [ 5120/10000]\n",
      "loss: 0.008804 [ 7680/10000]\n",
      "Test Avg loss: 0.006168\n",
      "\n",
      "Epoch 73\n",
      " ===========================\n",
      "loss: 0.004465 [    0/10000]\n",
      "loss: 0.007354 [ 2560/10000]\n",
      "loss: 0.005211 [ 5120/10000]\n",
      "loss: 0.004340 [ 7680/10000]\n",
      "Test Avg loss: 0.007126\n",
      "\n",
      "Epoch 74\n",
      " ===========================\n",
      "loss: 0.006733 [    0/10000]\n",
      "loss: 0.005431 [ 2560/10000]\n",
      "loss: 0.005452 [ 5120/10000]\n",
      "loss: 0.005854 [ 7680/10000]\n",
      "Test Avg loss: 0.007998\n",
      "\n",
      "Epoch 75\n",
      " ===========================\n",
      "loss: 0.007255 [    0/10000]\n",
      "loss: 0.005289 [ 2560/10000]\n",
      "loss: 0.007548 [ 5120/10000]\n",
      "loss: 0.004291 [ 7680/10000]\n",
      "Test Avg loss: 0.006666\n",
      "\n",
      "Epoch 76\n",
      " ===========================\n",
      "loss: 0.005872 [    0/10000]\n",
      "loss: 0.004744 [ 2560/10000]\n",
      "loss: 0.008312 [ 5120/10000]\n",
      "loss: 0.005805 [ 7680/10000]\n",
      "Test Avg loss: 0.007820\n",
      "\n",
      "Epoch 77\n",
      " ===========================\n",
      "loss: 0.007484 [    0/10000]\n",
      "loss: 0.008056 [ 2560/10000]\n",
      "loss: 0.004734 [ 5120/10000]\n",
      "loss: 0.007501 [ 7680/10000]\n",
      "Test Avg loss: 0.006022\n",
      "\n",
      "Epoch 78\n",
      " ===========================\n",
      "loss: 0.004780 [    0/10000]\n",
      "loss: 0.004146 [ 2560/10000]\n",
      "loss: 0.003832 [ 5120/10000]\n",
      "loss: 0.004368 [ 7680/10000]\n",
      "Test Avg loss: 0.005254\n",
      "\n",
      "Epoch 79\n",
      " ===========================\n",
      "loss: 0.004341 [    0/10000]\n",
      "loss: 0.007620 [ 2560/10000]\n",
      "loss: 0.005068 [ 5120/10000]\n",
      "loss: 0.005668 [ 7680/10000]\n",
      "Test Avg loss: 0.004920\n",
      "\n",
      "Epoch 80\n",
      " ===========================\n",
      "loss: 0.004098 [    0/10000]\n",
      "loss: 0.004398 [ 2560/10000]\n",
      "loss: 0.005253 [ 5120/10000]\n",
      "loss: 0.006343 [ 7680/10000]\n",
      "Test Avg loss: 0.004980\n",
      "\n",
      "Epoch 81\n",
      " ===========================\n",
      "loss: 0.003995 [    0/10000]\n",
      "loss: 0.004385 [ 2560/10000]\n",
      "loss: 0.005818 [ 5120/10000]\n",
      "loss: 0.006473 [ 7680/10000]\n",
      "Test Avg loss: 0.005118\n",
      "\n",
      "Epoch 82\n",
      " ===========================\n",
      "loss: 0.004691 [    0/10000]\n",
      "loss: 0.005005 [ 2560/10000]\n",
      "loss: 0.003600 [ 5120/10000]\n",
      "loss: 0.005490 [ 7680/10000]\n",
      "Test Avg loss: 0.005014\n",
      "\n",
      "Epoch 83\n",
      " ===========================\n",
      "loss: 0.004456 [    0/10000]\n",
      "loss: 0.006120 [ 2560/10000]\n",
      "loss: 0.004859 [ 5120/10000]\n",
      "loss: 0.004879 [ 7680/10000]\n",
      "Test Avg loss: 0.005013\n",
      "\n",
      "Epoch 84\n",
      " ===========================\n",
      "loss: 0.004990 [    0/10000]\n",
      "loss: 0.004700 [ 2560/10000]\n",
      "loss: 0.006412 [ 5120/10000]\n",
      "loss: 0.003706 [ 7680/10000]\n",
      "Test Avg loss: 0.006829\n",
      "\n",
      "Epoch 85\n",
      " ===========================\n",
      "loss: 0.006426 [    0/10000]\n",
      "loss: 0.004509 [ 2560/10000]\n",
      "loss: 0.004432 [ 5120/10000]\n",
      "loss: 0.003134 [ 7680/10000]\n",
      "Test Avg loss: 0.005132\n",
      "\n",
      "Epoch 86\n",
      " ===========================\n",
      "loss: 0.005413 [    0/10000]\n",
      "loss: 0.003143 [ 2560/10000]\n",
      "loss: 0.003861 [ 5120/10000]\n",
      "loss: 0.005974 [ 7680/10000]\n",
      "Test Avg loss: 0.007383\n",
      "\n",
      "Epoch 87\n",
      " ===========================\n",
      "loss: 0.006284 [    0/10000]\n",
      "loss: 0.007266 [ 2560/10000]\n",
      "loss: 0.004525 [ 5120/10000]\n",
      "loss: 0.006050 [ 7680/10000]\n",
      "Test Avg loss: 0.004782\n",
      "\n",
      "Epoch 88\n",
      " ===========================\n",
      "loss: 0.002730 [    0/10000]\n",
      "loss: 0.005938 [ 2560/10000]\n",
      "loss: 0.004780 [ 5120/10000]\n",
      "loss: 0.003681 [ 7680/10000]\n",
      "Test Avg loss: 0.005189\n",
      "\n",
      "Epoch 89\n",
      " ===========================\n",
      "loss: 0.005153 [    0/10000]\n",
      "loss: 0.004233 [ 2560/10000]\n",
      "loss: 0.004646 [ 5120/10000]\n",
      "loss: 0.003569 [ 7680/10000]\n",
      "Test Avg loss: 0.005584\n",
      "\n",
      "Epoch 90\n",
      " ===========================\n",
      "loss: 0.007864 [    0/10000]\n",
      "loss: 0.004082 [ 2560/10000]\n",
      "loss: 0.005730 [ 5120/10000]\n",
      "loss: 0.004128 [ 7680/10000]\n",
      "Test Avg loss: 0.006074\n",
      "\n",
      "Epoch 91\n",
      " ===========================\n",
      "loss: 0.005778 [    0/10000]\n",
      "loss: 0.004094 [ 2560/10000]\n",
      "loss: 0.004641 [ 5120/10000]\n",
      "loss: 0.004169 [ 7680/10000]\n",
      "Test Avg loss: 0.004382\n",
      "\n",
      "Epoch 92\n",
      " ===========================\n",
      "loss: 0.003635 [    0/10000]\n",
      "loss: 0.002936 [ 2560/10000]\n",
      "loss: 0.003063 [ 5120/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.004710 [ 7680/10000]\n",
      "Test Avg loss: 0.004286\n",
      "\n",
      "Epoch 93\n",
      " ===========================\n",
      "loss: 0.004161 [    0/10000]\n",
      "loss: 0.005117 [ 2560/10000]\n",
      "loss: 0.004403 [ 5120/10000]\n",
      "loss: 0.004042 [ 7680/10000]\n",
      "Test Avg loss: 0.004984\n",
      "\n",
      "Epoch 94\n",
      " ===========================\n",
      "loss: 0.004678 [    0/10000]\n",
      "loss: 0.004518 [ 2560/10000]\n",
      "loss: 0.006797 [ 5120/10000]\n",
      "loss: 0.004364 [ 7680/10000]\n",
      "Test Avg loss: 0.004311\n",
      "\n",
      "Epoch 95\n",
      " ===========================\n",
      "loss: 0.004370 [    0/10000]\n",
      "loss: 0.003929 [ 2560/10000]\n",
      "loss: 0.004590 [ 5120/10000]\n",
      "loss: 0.003588 [ 7680/10000]\n",
      "Test Avg loss: 0.005163\n",
      "\n",
      "Epoch 96\n",
      " ===========================\n",
      "loss: 0.005562 [    0/10000]\n",
      "loss: 0.004037 [ 2560/10000]\n",
      "loss: 0.003335 [ 5120/10000]\n",
      "loss: 0.005528 [ 7680/10000]\n",
      "Test Avg loss: 0.005168\n",
      "\n",
      "Epoch 97\n",
      " ===========================\n",
      "loss: 0.004505 [    0/10000]\n",
      "loss: 0.004449 [ 2560/10000]\n",
      "loss: 0.003460 [ 5120/10000]\n",
      "loss: 0.002693 [ 7680/10000]\n",
      "Test Avg loss: 0.008207\n",
      "\n",
      "Epoch 98\n",
      " ===========================\n",
      "loss: 0.006833 [    0/10000]\n",
      "loss: 0.004690 [ 2560/10000]\n",
      "loss: 0.004817 [ 5120/10000]\n",
      "loss: 0.005502 [ 7680/10000]\n",
      "Test Avg loss: 0.004405\n",
      "\n",
      "Epoch 99\n",
      " ===========================\n",
      "loss: 0.005151 [    0/10000]\n",
      "loss: 0.003661 [ 2560/10000]\n",
      "loss: 0.004644 [ 5120/10000]\n",
      "loss: 0.004171 [ 7680/10000]\n",
      "Test Avg loss: 0.004878\n",
      "\n",
      "Epoch 100\n",
      " ===========================\n",
      "loss: 0.004965 [    0/10000]\n",
      "loss: 0.003152 [ 2560/10000]\n",
      "loss: 0.004864 [ 5120/10000]\n",
      "loss: 0.004911 [ 7680/10000]\n",
      "Test Avg loss: 0.004317\n",
      "\n",
      "Epoch 101\n",
      " ===========================\n",
      "loss: 0.003838 [    0/10000]\n",
      "loss: 0.002814 [ 2560/10000]\n",
      "loss: 0.005758 [ 5120/10000]\n",
      "loss: 0.003582 [ 7680/10000]\n",
      "Test Avg loss: 0.003968\n",
      "\n",
      "Epoch 102\n",
      " ===========================\n",
      "loss: 0.004220 [    0/10000]\n",
      "loss: 0.003187 [ 2560/10000]\n",
      "loss: 0.003920 [ 5120/10000]\n",
      "loss: 0.003367 [ 7680/10000]\n",
      "Test Avg loss: 0.003520\n",
      "\n",
      "Epoch 103\n",
      " ===========================\n",
      "loss: 0.002609 [    0/10000]\n",
      "loss: 0.002934 [ 2560/10000]\n",
      "loss: 0.003340 [ 5120/10000]\n",
      "loss: 0.002700 [ 7680/10000]\n",
      "Test Avg loss: 0.004840\n",
      "\n",
      "Epoch 104\n",
      " ===========================\n",
      "loss: 0.004930 [    0/10000]\n",
      "loss: 0.005191 [ 2560/10000]\n",
      "loss: 0.004108 [ 5120/10000]\n",
      "loss: 0.003440 [ 7680/10000]\n",
      "Test Avg loss: 0.005318\n",
      "\n",
      "Epoch 105\n",
      " ===========================\n",
      "loss: 0.004231 [    0/10000]\n",
      "loss: 0.002651 [ 2560/10000]\n",
      "loss: 0.003828 [ 5120/10000]\n",
      "loss: 0.003791 [ 7680/10000]\n",
      "Test Avg loss: 0.004957\n",
      "\n",
      "Epoch 106\n",
      " ===========================\n",
      "loss: 0.005053 [    0/10000]\n",
      "loss: 0.003481 [ 2560/10000]\n",
      "loss: 0.002926 [ 5120/10000]\n",
      "loss: 0.002432 [ 7680/10000]\n",
      "Test Avg loss: 0.006600\n",
      "\n",
      "Epoch 107\n",
      " ===========================\n",
      "loss: 0.006318 [    0/10000]\n",
      "loss: 0.004282 [ 2560/10000]\n",
      "loss: 0.004202 [ 5120/10000]\n",
      "loss: 0.002794 [ 7680/10000]\n",
      "Test Avg loss: 0.004096\n",
      "\n",
      "Epoch 108\n",
      " ===========================\n",
      "loss: 0.004442 [    0/10000]\n",
      "loss: 0.003966 [ 2560/10000]\n",
      "loss: 0.003155 [ 5120/10000]\n",
      "loss: 0.002511 [ 7680/10000]\n",
      "Test Avg loss: 0.004677\n",
      "\n",
      "Epoch 109\n",
      " ===========================\n",
      "loss: 0.004126 [    0/10000]\n",
      "loss: 0.005135 [ 2560/10000]\n",
      "loss: 0.004656 [ 5120/10000]\n",
      "loss: 0.004064 [ 7680/10000]\n",
      "Test Avg loss: 0.004129\n",
      "\n",
      "Epoch 110\n",
      " ===========================\n",
      "loss: 0.003328 [    0/10000]\n",
      "loss: 0.003544 [ 2560/10000]\n",
      "loss: 0.003222 [ 5120/10000]\n",
      "loss: 0.003538 [ 7680/10000]\n",
      "Test Avg loss: 0.011513\n",
      "\n",
      "Epoch 111\n",
      " ===========================\n",
      "loss: 0.007860 [    0/10000]\n",
      "loss: 0.005261 [ 2560/10000]\n",
      "loss: 0.004944 [ 5120/10000]\n",
      "loss: 0.003739 [ 7680/10000]\n",
      "Test Avg loss: 0.004332\n",
      "\n",
      "Epoch 112\n",
      " ===========================\n",
      "loss: 0.004654 [    0/10000]\n",
      "loss: 0.004447 [ 2560/10000]\n",
      "loss: 0.004357 [ 5120/10000]\n",
      "loss: 0.003110 [ 7680/10000]\n",
      "Test Avg loss: 0.004327\n",
      "\n",
      "Epoch 113\n",
      " ===========================\n",
      "loss: 0.003895 [    0/10000]\n",
      "loss: 0.002326 [ 2560/10000]\n",
      "loss: 0.003934 [ 5120/10000]\n",
      "loss: 0.002853 [ 7680/10000]\n",
      "Test Avg loss: 0.004514\n",
      "\n",
      "Epoch 114\n",
      " ===========================\n",
      "loss: 0.003658 [    0/10000]\n",
      "loss: 0.003802 [ 2560/10000]\n",
      "loss: 0.004215 [ 5120/10000]\n",
      "loss: 0.003803 [ 7680/10000]\n",
      "Test Avg loss: 0.005226\n",
      "\n",
      "Epoch 115\n",
      " ===========================\n",
      "loss: 0.004913 [    0/10000]\n",
      "loss: 0.003375 [ 2560/10000]\n",
      "loss: 0.003520 [ 5120/10000]\n",
      "loss: 0.003816 [ 7680/10000]\n",
      "Test Avg loss: 0.003649\n",
      "\n",
      "Epoch 116\n",
      " ===========================\n",
      "loss: 0.003102 [    0/10000]\n",
      "loss: 0.002839 [ 2560/10000]\n",
      "loss: 0.002700 [ 5120/10000]\n",
      "loss: 0.003355 [ 7680/10000]\n",
      "Test Avg loss: 0.003342\n",
      "\n",
      "Epoch 117\n",
      " ===========================\n",
      "loss: 0.003139 [    0/10000]\n",
      "loss: 0.003145 [ 2560/10000]\n",
      "loss: 0.003598 [ 5120/10000]\n",
      "loss: 0.004271 [ 7680/10000]\n",
      "Test Avg loss: 0.003355\n",
      "\n",
      "Epoch 118\n",
      " ===========================\n",
      "loss: 0.002357 [    0/10000]\n",
      "loss: 0.002452 [ 2560/10000]\n",
      "loss: 0.002939 [ 5120/10000]\n",
      "loss: 0.003964 [ 7680/10000]\n",
      "Test Avg loss: 0.003649\n",
      "\n",
      "Epoch 119\n",
      " ===========================\n",
      "loss: 0.002954 [    0/10000]\n",
      "loss: 0.002634 [ 2560/10000]\n",
      "loss: 0.003504 [ 5120/10000]\n",
      "loss: 0.002372 [ 7680/10000]\n",
      "Test Avg loss: 0.004904\n",
      "\n",
      "Epoch 120\n",
      " ===========================\n",
      "loss: 0.004896 [    0/10000]\n",
      "loss: 0.005631 [ 2560/10000]\n",
      "loss: 0.002918 [ 5120/10000]\n",
      "loss: 0.002567 [ 7680/10000]\n",
      "Test Avg loss: 0.003978\n",
      "\n",
      "Epoch 121\n",
      " ===========================\n",
      "loss: 0.005125 [    0/10000]\n",
      "loss: 0.002999 [ 2560/10000]\n",
      "loss: 0.002544 [ 5120/10000]\n",
      "loss: 0.004328 [ 7680/10000]\n",
      "Test Avg loss: 0.004344\n",
      "\n",
      "Epoch 122\n",
      " ===========================\n",
      "loss: 0.003498 [    0/10000]\n",
      "loss: 0.002591 [ 2560/10000]\n",
      "loss: 0.002212 [ 5120/10000]\n",
      "loss: 0.003149 [ 7680/10000]\n",
      "Test Avg loss: 0.004181\n",
      "\n",
      "Epoch 123\n",
      " ===========================\n",
      "loss: 0.003620 [    0/10000]\n",
      "loss: 0.003890 [ 2560/10000]\n",
      "loss: 0.003696 [ 5120/10000]\n",
      "loss: 0.002473 [ 7680/10000]\n",
      "Test Avg loss: 0.004015\n",
      "\n",
      "Epoch 124\n",
      " ===========================\n",
      "loss: 0.002863 [    0/10000]\n",
      "loss: 0.003686 [ 2560/10000]\n",
      "loss: 0.002608 [ 5120/10000]\n",
      "loss: 0.002343 [ 7680/10000]\n",
      "Test Avg loss: 0.003776\n",
      "\n",
      "Epoch 125\n",
      " ===========================\n",
      "loss: 0.003336 [    0/10000]\n",
      "loss: 0.002721 [ 2560/10000]\n",
      "loss: 0.004103 [ 5120/10000]\n",
      "loss: 0.002500 [ 7680/10000]\n",
      "Test Avg loss: 0.004786\n",
      "\n",
      "Epoch 126\n",
      " ===========================\n",
      "loss: 0.006328 [    0/10000]\n",
      "loss: 0.004264 [ 2560/10000]\n",
      "loss: 0.003020 [ 5120/10000]\n",
      "loss: 0.003742 [ 7680/10000]\n",
      "Test Avg loss: 0.005866\n",
      "\n",
      "Epoch 127\n",
      " ===========================\n",
      "loss: 0.005898 [    0/10000]\n",
      "loss: 0.003392 [ 2560/10000]\n",
      "loss: 0.001946 [ 5120/10000]\n",
      "loss: 0.002797 [ 7680/10000]\n",
      "Test Avg loss: 0.005256\n",
      "\n",
      "Epoch 128\n",
      " ===========================\n",
      "loss: 0.005272 [    0/10000]\n",
      "loss: 0.002566 [ 2560/10000]\n",
      "loss: 0.002406 [ 5120/10000]\n",
      "loss: 0.002731 [ 7680/10000]\n",
      "Test Avg loss: 0.005453\n",
      "\n",
      "Epoch 129\n",
      " ===========================\n",
      "loss: 0.005642 [    0/10000]\n",
      "loss: 0.002227 [ 2560/10000]\n",
      "loss: 0.002528 [ 5120/10000]\n",
      "loss: 0.002820 [ 7680/10000]\n",
      "Test Avg loss: 0.003484\n",
      "\n",
      "Epoch 130\n",
      " ===========================\n",
      "loss: 0.003538 [    0/10000]\n",
      "loss: 0.002406 [ 2560/10000]\n",
      "loss: 0.003271 [ 5120/10000]\n",
      "loss: 0.002290 [ 7680/10000]\n",
      "Test Avg loss: 0.002596\n",
      "\n",
      "Epoch 131\n",
      " ===========================\n",
      "loss: 0.002378 [    0/10000]\n",
      "loss: 0.002176 [ 2560/10000]\n",
      "loss: 0.003019 [ 5120/10000]\n",
      "loss: 0.002505 [ 7680/10000]\n",
      "Test Avg loss: 0.003540\n",
      "\n",
      "Epoch 132\n",
      " ===========================\n",
      "loss: 0.003812 [    0/10000]\n",
      "loss: 0.002754 [ 2560/10000]\n",
      "loss: 0.002283 [ 5120/10000]\n",
      "loss: 0.002503 [ 7680/10000]\n",
      "Test Avg loss: 0.004776\n",
      "\n",
      "Epoch 133\n",
      " ===========================\n",
      "loss: 0.004816 [    0/10000]\n",
      "loss: 0.002735 [ 2560/10000]\n",
      "loss: 0.002986 [ 5120/10000]\n",
      "loss: 0.002651 [ 7680/10000]\n",
      "Test Avg loss: 0.003753\n",
      "\n",
      "Epoch 134\n",
      " ===========================\n",
      "loss: 0.003582 [    0/10000]\n",
      "loss: 0.002898 [ 2560/10000]\n",
      "loss: 0.002501 [ 5120/10000]\n",
      "loss: 0.001842 [ 7680/10000]\n",
      "Test Avg loss: 0.003544\n",
      "\n",
      "Epoch 135\n",
      " ===========================\n",
      "loss: 0.002983 [    0/10000]\n",
      "loss: 0.002718 [ 2560/10000]\n",
      "loss: 0.002081 [ 5120/10000]\n",
      "loss: 0.002576 [ 7680/10000]\n",
      "Test Avg loss: 0.003028\n",
      "\n",
      "Epoch 136\n",
      " ===========================\n",
      "loss: 0.002727 [    0/10000]\n",
      "loss: 0.002222 [ 2560/10000]\n",
      "loss: 0.003432 [ 5120/10000]\n",
      "loss: 0.002146 [ 7680/10000]\n",
      "Test Avg loss: 0.003213\n",
      "\n",
      "Epoch 137\n",
      " ===========================\n",
      "loss: 0.003800 [    0/10000]\n",
      "loss: 0.003701 [ 2560/10000]\n",
      "loss: 0.002735 [ 5120/10000]\n",
      "loss: 0.002481 [ 7680/10000]\n",
      "Test Avg loss: 0.004758\n",
      "\n",
      "Epoch 138\n",
      " ===========================\n",
      "loss: 0.005199 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.004069 [ 2560/10000]\n",
      "loss: 0.002347 [ 5120/10000]\n",
      "loss: 0.003106 [ 7680/10000]\n",
      "Test Avg loss: 0.004346\n",
      "\n",
      "Epoch 139\n",
      " ===========================\n",
      "loss: 0.004464 [    0/10000]\n",
      "loss: 0.003969 [ 2560/10000]\n",
      "loss: 0.001904 [ 5120/10000]\n",
      "loss: 0.003298 [ 7680/10000]\n",
      "Test Avg loss: 0.003836\n",
      "\n",
      "Epoch 140\n",
      " ===========================\n",
      "loss: 0.003841 [    0/10000]\n",
      "loss: 0.002085 [ 2560/10000]\n",
      "loss: 0.002724 [ 5120/10000]\n",
      "loss: 0.002976 [ 7680/10000]\n",
      "Test Avg loss: 0.002959\n",
      "\n",
      "Epoch 141\n",
      " ===========================\n",
      "loss: 0.002442 [    0/10000]\n",
      "loss: 0.002252 [ 2560/10000]\n",
      "loss: 0.002441 [ 5120/10000]\n",
      "loss: 0.002963 [ 7680/10000]\n",
      "Test Avg loss: 0.006476\n",
      "\n",
      "Epoch 142\n",
      " ===========================\n",
      "loss: 0.006460 [    0/10000]\n",
      "loss: 0.003846 [ 2560/10000]\n",
      "loss: 0.002822 [ 5120/10000]\n",
      "loss: 0.002707 [ 7680/10000]\n",
      "Test Avg loss: 0.003722\n",
      "\n",
      "Epoch 143\n",
      " ===========================\n",
      "loss: 0.003243 [    0/10000]\n",
      "loss: 0.002310 [ 2560/10000]\n",
      "loss: 0.002930 [ 5120/10000]\n",
      "loss: 0.003098 [ 7680/10000]\n",
      "Test Avg loss: 0.006885\n",
      "\n",
      "Epoch 144\n",
      " ===========================\n",
      "loss: 0.006092 [    0/10000]\n",
      "loss: 0.005001 [ 2560/10000]\n",
      "loss: 0.002821 [ 5120/10000]\n",
      "loss: 0.003060 [ 7680/10000]\n",
      "Test Avg loss: 0.003588\n",
      "\n",
      "Epoch 145\n",
      " ===========================\n",
      "loss: 0.002975 [    0/10000]\n",
      "loss: 0.002795 [ 2560/10000]\n",
      "loss: 0.003211 [ 5120/10000]\n",
      "loss: 0.002937 [ 7680/10000]\n",
      "Test Avg loss: 0.002964\n",
      "\n",
      "Epoch 146\n",
      " ===========================\n",
      "loss: 0.004702 [    0/10000]\n",
      "loss: 0.002758 [ 2560/10000]\n",
      "loss: 0.002710 [ 5120/10000]\n",
      "loss: 0.003940 [ 7680/10000]\n",
      "Test Avg loss: 0.005529\n",
      "\n",
      "Epoch 147\n",
      " ===========================\n",
      "loss: 0.004494 [    0/10000]\n",
      "loss: 0.003601 [ 2560/10000]\n",
      "loss: 0.003104 [ 5120/10000]\n",
      "loss: 0.001942 [ 7680/10000]\n",
      "Test Avg loss: 0.005290\n",
      "\n",
      "Epoch 148\n",
      " ===========================\n",
      "loss: 0.006152 [    0/10000]\n",
      "loss: 0.002206 [ 2560/10000]\n",
      "loss: 0.002623 [ 5120/10000]\n",
      "loss: 0.002745 [ 7680/10000]\n",
      "Test Avg loss: 0.003510\n",
      "\n",
      "Epoch 149\n",
      " ===========================\n",
      "loss: 0.002985 [    0/10000]\n",
      "loss: 0.002411 [ 2560/10000]\n",
      "loss: 0.005706 [ 5120/10000]\n",
      "loss: 0.004255 [ 7680/10000]\n",
      "Test Avg loss: 0.003355\n",
      "\n",
      "Epoch 150\n",
      " ===========================\n",
      "loss: 0.002846 [    0/10000]\n",
      "loss: 0.002975 [ 2560/10000]\n",
      "loss: 0.002320 [ 5120/10000]\n",
      "loss: 0.003152 [ 7680/10000]\n",
      "Test Avg loss: 0.002831\n",
      "\n",
      "Epoch 151\n",
      " ===========================\n",
      "loss: 0.002095 [    0/10000]\n",
      "loss: 0.002454 [ 2560/10000]\n",
      "loss: 0.002384 [ 5120/10000]\n",
      "loss: 0.001836 [ 7680/10000]\n",
      "Test Avg loss: 0.004554\n",
      "\n",
      "Epoch 152\n",
      " ===========================\n",
      "loss: 0.004411 [    0/10000]\n",
      "loss: 0.002841 [ 2560/10000]\n",
      "loss: 0.003642 [ 5120/10000]\n",
      "loss: 0.002059 [ 7680/10000]\n",
      "Test Avg loss: 0.003770\n",
      "\n",
      "Epoch 153\n",
      " ===========================\n",
      "loss: 0.004196 [    0/10000]\n",
      "loss: 0.002081 [ 2560/10000]\n",
      "loss: 0.002239 [ 5120/10000]\n",
      "loss: 0.003075 [ 7680/10000]\n",
      "Test Avg loss: 0.005174\n",
      "\n",
      "Epoch 154\n",
      " ===========================\n",
      "loss: 0.004097 [    0/10000]\n",
      "loss: 0.002477 [ 2560/10000]\n",
      "loss: 0.002265 [ 5120/10000]\n",
      "loss: 0.003984 [ 7680/10000]\n",
      "Test Avg loss: 0.004362\n",
      "\n",
      "Epoch 155\n",
      " ===========================\n",
      "loss: 0.004218 [    0/10000]\n",
      "loss: 0.002316 [ 2560/10000]\n",
      "loss: 0.002060 [ 5120/10000]\n",
      "loss: 0.002339 [ 7680/10000]\n",
      "Test Avg loss: 0.003203\n",
      "\n",
      "Epoch 156\n",
      " ===========================\n",
      "loss: 0.002953 [    0/10000]\n",
      "loss: 0.002766 [ 2560/10000]\n",
      "loss: 0.003041 [ 5120/10000]\n",
      "loss: 0.001641 [ 7680/10000]\n",
      "Test Avg loss: 0.003763\n",
      "\n",
      "Epoch 157\n",
      " ===========================\n",
      "loss: 0.003078 [    0/10000]\n",
      "loss: 0.002129 [ 2560/10000]\n",
      "loss: 0.004037 [ 5120/10000]\n",
      "loss: 0.002172 [ 7680/10000]\n",
      "Test Avg loss: 0.003017\n",
      "\n",
      "Epoch 158\n",
      " ===========================\n",
      "loss: 0.002667 [    0/10000]\n",
      "loss: 0.002139 [ 2560/10000]\n",
      "loss: 0.002302 [ 5120/10000]\n",
      "loss: 0.003168 [ 7680/10000]\n",
      "Test Avg loss: 0.003750\n",
      "\n",
      "Epoch 159\n",
      " ===========================\n",
      "loss: 0.004563 [    0/10000]\n",
      "loss: 0.001937 [ 2560/10000]\n",
      "loss: 0.002189 [ 5120/10000]\n",
      "loss: 0.002786 [ 7680/10000]\n",
      "Test Avg loss: 0.002504\n",
      "\n",
      "Epoch 160\n",
      " ===========================\n",
      "loss: 0.001911 [    0/10000]\n",
      "loss: 0.002804 [ 2560/10000]\n",
      "loss: 0.002439 [ 5120/10000]\n",
      "loss: 0.002597 [ 7680/10000]\n",
      "Test Avg loss: 0.002712\n",
      "\n",
      "Epoch 161\n",
      " ===========================\n",
      "loss: 0.003089 [    0/10000]\n",
      "loss: 0.001734 [ 2560/10000]\n",
      "loss: 0.001827 [ 5120/10000]\n",
      "loss: 0.002107 [ 7680/10000]\n",
      "Test Avg loss: 0.007677\n",
      "\n",
      "Epoch 162\n",
      " ===========================\n",
      "loss: 0.007601 [    0/10000]\n",
      "loss: 0.029375 [ 2560/10000]\n",
      "loss: 0.019377 [ 5120/10000]\n",
      "loss: 0.012492 [ 7680/10000]\n",
      "Test Avg loss: 0.012371\n",
      "\n",
      "Epoch 163\n",
      " ===========================\n",
      "loss: 0.013398 [    0/10000]\n",
      "loss: 0.005846 [ 2560/10000]\n",
      "loss: 0.005457 [ 5120/10000]\n",
      "loss: 0.009058 [ 7680/10000]\n",
      "Test Avg loss: 0.055003\n",
      "\n",
      "Epoch 164\n",
      " ===========================\n",
      "loss: 0.051381 [    0/10000]\n",
      "loss: 0.036596 [ 2560/10000]\n",
      "loss: 0.025046 [ 5120/10000]\n",
      "loss: 0.011466 [ 7680/10000]\n",
      "Test Avg loss: 0.008221\n",
      "\n",
      "Epoch 165\n",
      " ===========================\n",
      "loss: 0.007914 [    0/10000]\n",
      "loss: 0.006966 [ 2560/10000]\n",
      "loss: 0.006681 [ 5120/10000]\n",
      "loss: 0.005741 [ 7680/10000]\n",
      "Test Avg loss: 0.005334\n",
      "\n",
      "Epoch 166\n",
      " ===========================\n",
      "loss: 0.004047 [    0/10000]\n",
      "loss: 0.004736 [ 2560/10000]\n",
      "loss: 0.003948 [ 5120/10000]\n",
      "loss: 0.003827 [ 7680/10000]\n",
      "Test Avg loss: 0.004249\n",
      "\n",
      "Epoch 167\n",
      " ===========================\n",
      "loss: 0.004177 [    0/10000]\n",
      "loss: 0.003498 [ 2560/10000]\n",
      "loss: 0.004018 [ 5120/10000]\n",
      "loss: 0.002995 [ 7680/10000]\n",
      "Test Avg loss: 0.004010\n",
      "\n",
      "Epoch 168\n",
      " ===========================\n",
      "loss: 0.003971 [    0/10000]\n",
      "loss: 0.002891 [ 2560/10000]\n",
      "loss: 0.003374 [ 5120/10000]\n",
      "loss: 0.004904 [ 7680/10000]\n",
      "Test Avg loss: 0.003957\n",
      "\n",
      "Epoch 169\n",
      " ===========================\n",
      "loss: 0.004050 [    0/10000]\n",
      "loss: 0.002793 [ 2560/10000]\n",
      "loss: 0.003077 [ 5120/10000]\n",
      "loss: 0.002777 [ 7680/10000]\n",
      "Test Avg loss: 0.003416\n",
      "\n",
      "Epoch 170\n",
      " ===========================\n",
      "loss: 0.003657 [    0/10000]\n",
      "loss: 0.003001 [ 2560/10000]\n",
      "loss: 0.003079 [ 5120/10000]\n",
      "loss: 0.002906 [ 7680/10000]\n",
      "Test Avg loss: 0.003775\n",
      "\n",
      "Epoch 171\n",
      " ===========================\n",
      "loss: 0.003619 [    0/10000]\n",
      "loss: 0.002310 [ 2560/10000]\n",
      "loss: 0.002927 [ 5120/10000]\n",
      "loss: 0.002906 [ 7680/10000]\n",
      "Test Avg loss: 0.003020\n",
      "\n",
      "Epoch 172\n",
      " ===========================\n",
      "loss: 0.002573 [    0/10000]\n",
      "loss: 0.002103 [ 2560/10000]\n",
      "loss: 0.002581 [ 5120/10000]\n",
      "loss: 0.002650 [ 7680/10000]\n",
      "Test Avg loss: 0.002859\n",
      "\n",
      "Epoch 173\n",
      " ===========================\n",
      "loss: 0.002463 [    0/10000]\n",
      "loss: 0.002524 [ 2560/10000]\n",
      "loss: 0.002546 [ 5120/10000]\n",
      "loss: 0.002711 [ 7680/10000]\n",
      "Test Avg loss: 0.002906\n",
      "\n",
      "Epoch 174\n",
      " ===========================\n",
      "loss: 0.002230 [    0/10000]\n",
      "loss: 0.003125 [ 2560/10000]\n",
      "loss: 0.002569 [ 5120/10000]\n",
      "loss: 0.002115 [ 7680/10000]\n",
      "Test Avg loss: 0.003037\n",
      "\n",
      "Epoch 175\n",
      " ===========================\n",
      "loss: 0.002720 [    0/10000]\n",
      "loss: 0.003754 [ 2560/10000]\n",
      "loss: 0.002110 [ 5120/10000]\n",
      "loss: 0.003349 [ 7680/10000]\n",
      "Test Avg loss: 0.003549\n",
      "\n",
      "Epoch 176\n",
      " ===========================\n",
      "loss: 0.002794 [    0/10000]\n",
      "loss: 0.002815 [ 2560/10000]\n",
      "loss: 0.002499 [ 5120/10000]\n",
      "loss: 0.002406 [ 7680/10000]\n",
      "Test Avg loss: 0.002714\n",
      "\n",
      "Epoch 177\n",
      " ===========================\n",
      "loss: 0.002007 [    0/10000]\n",
      "loss: 0.002517 [ 2560/10000]\n",
      "loss: 0.002653 [ 5120/10000]\n",
      "loss: 0.002481 [ 7680/10000]\n",
      "Test Avg loss: 0.002862\n",
      "\n",
      "Epoch 178\n",
      " ===========================\n",
      "loss: 0.003368 [    0/10000]\n",
      "loss: 0.002756 [ 2560/10000]\n",
      "loss: 0.001922 [ 5120/10000]\n",
      "loss: 0.002057 [ 7680/10000]\n",
      "Test Avg loss: 0.002775\n",
      "\n",
      "Epoch 179\n",
      " ===========================\n",
      "loss: 0.002136 [    0/10000]\n",
      "loss: 0.003274 [ 2560/10000]\n",
      "loss: 0.002806 [ 5120/10000]\n",
      "loss: 0.002040 [ 7680/10000]\n",
      "Test Avg loss: 0.003312\n",
      "\n",
      "Epoch 180\n",
      " ===========================\n",
      "loss: 0.002837 [    0/10000]\n",
      "loss: 0.002812 [ 2560/10000]\n",
      "loss: 0.002357 [ 5120/10000]\n",
      "loss: 0.002841 [ 7680/10000]\n",
      "Test Avg loss: 0.003080\n",
      "\n",
      "Epoch 181\n",
      " ===========================\n",
      "loss: 0.002821 [    0/10000]\n",
      "loss: 0.002413 [ 2560/10000]\n",
      "loss: 0.002531 [ 5120/10000]\n",
      "loss: 0.001995 [ 7680/10000]\n",
      "Test Avg loss: 0.003983\n",
      "\n",
      "Epoch 182\n",
      " ===========================\n",
      "loss: 0.004126 [    0/10000]\n",
      "loss: 0.002451 [ 2560/10000]\n",
      "loss: 0.001708 [ 5120/10000]\n",
      "loss: 0.002223 [ 7680/10000]\n",
      "Test Avg loss: 0.003845\n",
      "\n",
      "Epoch 183\n",
      " ===========================\n",
      "loss: 0.002707 [    0/10000]\n",
      "loss: 0.001444 [ 2560/10000]\n",
      "loss: 0.002255 [ 5120/10000]\n",
      "loss: 0.002269 [ 7680/10000]\n",
      "Test Avg loss: 0.002525\n",
      "\n",
      "Epoch 184\n",
      " ===========================\n",
      "loss: 0.003050 [    0/10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.001886 [ 2560/10000]\n",
      "loss: 0.004112 [ 5120/10000]\n",
      "loss: 0.003018 [ 7680/10000]\n",
      "Test Avg loss: 0.002841\n",
      "\n",
      "Epoch 185\n",
      " ===========================\n",
      "loss: 0.001789 [    0/10000]\n",
      "loss: 0.001771 [ 2560/10000]\n",
      "loss: 0.002687 [ 5120/10000]\n",
      "loss: 0.002786 [ 7680/10000]\n",
      "Test Avg loss: 0.002624\n",
      "\n",
      "Epoch 186\n",
      " ===========================\n",
      "loss: 0.002461 [    0/10000]\n",
      "loss: 0.001969 [ 2560/10000]\n",
      "loss: 0.002545 [ 5120/10000]\n",
      "loss: 0.003351 [ 7680/10000]\n",
      "Test Avg loss: 0.002938\n",
      "\n",
      "Epoch 187\n",
      " ===========================\n",
      "loss: 0.002342 [    0/10000]\n",
      "loss: 0.002147 [ 2560/10000]\n",
      "loss: 0.002417 [ 5120/10000]\n",
      "loss: 0.002510 [ 7680/10000]\n",
      "Test Avg loss: 0.002395\n",
      "\n",
      "Epoch 188\n",
      " ===========================\n",
      "loss: 0.001634 [    0/10000]\n",
      "loss: 0.002440 [ 2560/10000]\n",
      "loss: 0.002676 [ 5120/10000]\n",
      "loss: 0.001683 [ 7680/10000]\n",
      "Test Avg loss: 0.002669\n",
      "\n",
      "Epoch 189\n",
      " ===========================\n",
      "loss: 0.002251 [    0/10000]\n",
      "loss: 0.002136 [ 2560/10000]\n",
      "loss: 0.002772 [ 5120/10000]\n",
      "loss: 0.001673 [ 7680/10000]\n",
      "Test Avg loss: 0.002599\n",
      "\n",
      "Epoch 190\n",
      " ===========================\n",
      "loss: 0.002793 [    0/10000]\n",
      "loss: 0.002167 [ 2560/10000]\n",
      "loss: 0.002067 [ 5120/10000]\n",
      "loss: 0.002162 [ 7680/10000]\n",
      "Test Avg loss: 0.002642\n",
      "\n",
      "Epoch 191\n",
      " ===========================\n",
      "loss: 0.001851 [    0/10000]\n",
      "loss: 0.003023 [ 2560/10000]\n",
      "loss: 0.002520 [ 5120/10000]\n",
      "loss: 0.002105 [ 7680/10000]\n",
      "Test Avg loss: 0.003000\n",
      "\n",
      "Epoch 192\n",
      " ===========================\n",
      "loss: 0.002367 [    0/10000]\n",
      "loss: 0.004469 [ 2560/10000]\n",
      "loss: 0.002029 [ 5120/10000]\n",
      "loss: 0.002584 [ 7680/10000]\n",
      "Test Avg loss: 0.003835\n",
      "\n",
      "Epoch 193\n",
      " ===========================\n",
      "loss: 0.003765 [    0/10000]\n",
      "loss: 0.001497 [ 2560/10000]\n",
      "loss: 0.001225 [ 5120/10000]\n",
      "loss: 0.002324 [ 7680/10000]\n",
      "Test Avg loss: 0.002274\n",
      "\n",
      "Epoch 194\n",
      " ===========================\n",
      "loss: 0.002087 [    0/10000]\n",
      "loss: 0.002100 [ 2560/10000]\n",
      "loss: 0.002623 [ 5120/10000]\n",
      "loss: 0.001422 [ 7680/10000]\n",
      "Test Avg loss: 0.003151\n",
      "\n",
      "Epoch 195\n",
      " ===========================\n",
      "loss: 0.002840 [    0/10000]\n",
      "loss: 0.001610 [ 2560/10000]\n",
      "loss: 0.002371 [ 5120/10000]\n",
      "loss: 0.002053 [ 7680/10000]\n",
      "Test Avg loss: 0.002223\n",
      "\n",
      "Epoch 196\n",
      " ===========================\n",
      "loss: 0.001663 [    0/10000]\n",
      "loss: 0.003735 [ 2560/10000]\n",
      "loss: 0.002956 [ 5120/10000]\n",
      "loss: 0.002193 [ 7680/10000]\n",
      "Test Avg loss: 0.004486\n",
      "\n",
      "Epoch 197\n",
      " ===========================\n",
      "loss: 0.003830 [    0/10000]\n",
      "loss: 0.003339 [ 2560/10000]\n",
      "loss: 0.002447 [ 5120/10000]\n",
      "loss: 0.002395 [ 7680/10000]\n",
      "Test Avg loss: 0.002711\n",
      "\n",
      "Epoch 198\n",
      " ===========================\n",
      "loss: 0.002315 [    0/10000]\n",
      "loss: 0.001546 [ 2560/10000]\n",
      "loss: 0.002620 [ 5120/10000]\n",
      "loss: 0.001482 [ 7680/10000]\n",
      "Test Avg loss: 0.002933\n",
      "\n",
      "Epoch 199\n",
      " ===========================\n",
      "loss: 0.002497 [    0/10000]\n",
      "loss: 0.001562 [ 2560/10000]\n",
      "loss: 0.001978 [ 5120/10000]\n",
      "loss: 0.001999 [ 7680/10000]\n",
      "Test Avg loss: 0.002447\n",
      "\n",
      "Epoch 200\n",
      " ===========================\n",
      "loss: 0.002014 [    0/10000]\n",
      "loss: 0.001177 [ 2560/10000]\n",
      "loss: 0.001620 [ 5120/10000]\n",
      "loss: 0.001646 [ 7680/10000]\n",
      "Test Avg loss: 0.002437\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size, epochs, lr = 128, 200, 1e-3\n",
    "model = Autoencoder_Conv1D_deep().to(device=try_gpu())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validate_dataloader = DataLoader(validate_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n ===========================\")\n",
    "    training_loop(train_dataloader, model, loss_fn, optimizer, try_gpu())\n",
    "    test_loop(validate_dataloader, model, loss_fn, try_gpu())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62aa49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder_Conv1D_deep().to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d3b7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426df64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
